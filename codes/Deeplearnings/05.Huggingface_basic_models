{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets huggingface_hub fsspec==2024.10.0 -qqq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:54:07.600765Z","iopub.execute_input":"2025-02-04T07:54:07.601185Z","iopub.status.idle":"2025-02-04T07:54:10.910864Z","shell.execute_reply.started":"2025-02-04T07:54:07.601149Z","shell.execute_reply":"2025-02-04T07:54:10.909584Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Hugging Face 사용 이유\n- 여러 회사들이 개발한 모델이나 제공하는 데이터셋을 편리하게 사용하는 인터페이스 제공\n- ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:54:10.912546Z","iopub.execute_input":"2025-02-04T07:54:10.912823Z","iopub.status.idle":"2025-02-04T07:54:10.917246Z","shell.execute_reply.started":"2025-02-04T07:54:10.912796Z","shell.execute_reply":"2025-02-04T07:54:10.916409Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"bert_model = AutoModel.from_pretrained('google-bert/bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:54:10.918743Z","iopub.execute_input":"2025-02-04T07:54:10.919022Z","iopub.status.idle":"2025-02-04T07:54:11.041570Z","shell.execute_reply.started":"2025-02-04T07:54:10.918999Z","shell.execute_reply":"2025-02-04T07:54:11.040792Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"bert_tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:54:11.042642Z","iopub.execute_input":"2025-02-04T07:54:11.042855Z","iopub.status.idle":"2025-02-04T07:54:11.136715Z","shell.execute_reply.started":"2025-02-04T07:54:11.042837Z","shell.execute_reply":"2025-02-04T07:54:11.136108Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"type(bert_model), type(bert_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:54:11.137399Z","iopub.execute_input":"2025-02-04T07:54:11.137586Z","iopub.status.idle":"2025-02-04T07:54:11.142801Z","shell.execute_reply.started":"2025-02-04T07:54:11.137570Z","shell.execute_reply":"2025-02-04T07:54:11.141987Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(transformers.models.bert.modeling_bert.BertModel,\n transformers.models.bert.tokenization_bert_fast.BertTokenizerFast)"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"text = 'What is Huggingface Transformer?'\nencoded_input = bert_tokenizer(text, return_tensors='pt') # 학습 완료된 형태에 벡터로 결과 리턴\nbert_out = bert_model(**encoded_input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:54:11.143537Z","iopub.execute_input":"2025-02-04T07:54:11.143797Z","iopub.status.idle":"2025-02-04T07:54:11.221301Z","shell.execute_reply.started":"2025-02-04T07:54:11.143777Z","shell.execute_reply":"2025-02-04T07:54:11.220604Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# type(encoded_input), encoded_input\ntype(bert_out), bert_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:54:11.222183Z","iopub.execute_input":"2025-02-04T07:54:11.222519Z","iopub.status.idle":"2025-02-04T07:54:11.235942Z","shell.execute_reply.started":"2025-02-04T07:54:11.222486Z","shell.execute_reply":"2025-02-04T07:54:11.235090Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions,\n BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3060, -0.2428,  0.2329,  ..., -0.5468,  0.3700,  0.6556],\n          [-0.2695, -0.4481, -0.1817,  ...,  0.2393,  0.3194,  0.1278],\n          [-0.3174, -0.7152,  0.5921,  ..., -0.3882, -0.0896,  1.1300],\n          ...,\n          [-0.4150, -0.5087,  0.1999,  ..., -0.9073, -0.4628, -0.3216],\n          [-0.1996, -0.4657, -0.4558,  ..., -0.2395,  0.2295,  0.0486],\n          [ 0.7025,  0.1803, -0.1128,  ...,  0.4265, -0.3938, -0.4393]]],\n        grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.8942, -0.3405, -0.6064,  0.7698,  0.3569, -0.2487,  0.9001,  0.3037,\n          -0.5063, -1.0000,  0.0181,  0.7210,  0.9755,  0.4160,  0.9483, -0.7900,\n          -0.2471, -0.6251,  0.3025, -0.6405,  0.6593,  0.9995,  0.3704,  0.3298,\n           0.4709,  0.8558, -0.7759,  0.9301,  0.9559,  0.7418, -0.7657,  0.1653,\n          -0.9854, -0.2005, -0.6867, -0.9846,  0.3198, -0.7821,  0.0126,  0.0318,\n          -0.8692,  0.1535,  0.9999, -0.6748,  0.1767, -0.3719, -1.0000,  0.3010,\n          -0.9015,  0.5432,  0.6190,  0.2259,  0.1296,  0.3846,  0.4946,  0.0235,\n          -0.1628,  0.0582, -0.2881, -0.5322, -0.6438,  0.4448, -0.5118, -0.9078,\n           0.5423,  0.6597, -0.1457, -0.3155, -0.0936, -0.0827,  0.8912,  0.2424,\n           0.2200, -0.8550,  0.2931,  0.2605, -0.6666,  1.0000, -0.5541, -0.9688,\n           0.5527,  0.6067,  0.6036, -0.0041,  0.4913, -1.0000,  0.3396, -0.0512,\n          -0.9877,  0.2127,  0.4355, -0.1844,  0.6033,  0.5948, -0.6732, -0.2437,\n          -0.1591, -0.4730, -0.1872, -0.0912,  0.0949, -0.2516, -0.3445, -0.3702,\n           0.2303, -0.4320, -0.5456,  0.4804,  0.0517,  0.6510,  0.5097, -0.3249,\n           0.4168, -0.9494,  0.5309, -0.3772, -0.9822, -0.6416, -0.9809,  0.7236,\n          -0.0146, -0.3006,  0.9574,  0.1490,  0.3557, -0.0976, -0.4898, -1.0000,\n          -0.2690, -0.3824, -0.1067, -0.1887, -0.9693, -0.9531,  0.5907,  0.9554,\n           0.2226,  0.9996, -0.1767,  0.9323, -0.0624, -0.5723,  0.0612, -0.4742,\n           0.6312,  0.2158, -0.7259,  0.2350, -0.0783, -0.1216, -0.5069, -0.1975,\n          -0.4305, -0.9452, -0.3274,  0.9553, -0.2425, -0.7907,  0.0546, -0.2097,\n          -0.3272,  0.8675,  0.4904,  0.3267, -0.3465,  0.3354,  0.3512,  0.3933,\n          -0.8605,  0.2714,  0.4318, -0.3282, -0.6579, -0.9727, -0.3130,  0.5125,\n           0.9862,  0.7681,  0.3303,  0.3243, -0.3088,  0.2833, -0.9494,  0.9726,\n          -0.1320,  0.2036, -0.0209,  0.3715, -0.8676,  0.1469,  0.8383, -0.1009,\n          -0.8584,  0.0164, -0.4993, -0.4252, -0.5044,  0.4707, -0.2635, -0.3296,\n           0.1060,  0.9245,  0.9776,  0.7034, -0.0873,  0.5595, -0.8806, -0.4469,\n           0.0731,  0.1992,  0.1167,  0.9897, -0.5573, -0.1038, -0.9218, -0.9777,\n          -0.0530, -0.8700, -0.0480, -0.6318,  0.5317,  0.1753,  0.2522,  0.3503,\n          -0.9814, -0.8100,  0.3641, -0.3230,  0.4673, -0.2283,  0.5979,  0.7596,\n          -0.6584,  0.8259,  0.9291, -0.6043, -0.7574,  0.8583, -0.2859,  0.8980,\n          -0.5775,  0.9879,  0.5340,  0.5139, -0.8985, -0.6083, -0.9005, -0.4716,\n           0.0193,  0.0409,  0.7076,  0.6737,  0.3474,  0.2119, -0.5823,  0.9960,\n          -0.7518, -0.9523,  0.1204, -0.0624, -0.9860,  0.5187,  0.3357, -0.1512,\n          -0.4643, -0.6025, -0.9568,  0.8865,  0.0853,  0.9880, -0.2015, -0.9111,\n          -0.5341, -0.9240, -0.2174, -0.2356,  0.1082, -0.1420, -0.9544,  0.5002,\n           0.5658,  0.4237, -0.6549,  0.9974,  1.0000,  0.9617,  0.9036,  0.9005,\n          -0.9974, -0.4767,  1.0000, -0.9415, -1.0000, -0.9452, -0.6130,  0.3527,\n          -1.0000, -0.2883,  0.0632, -0.8864,  0.4175,  0.9715,  0.9887, -1.0000,\n           0.8391,  0.9119, -0.7004,  0.7091, -0.4105,  0.9665,  0.6239,  0.3954,\n          -0.2600,  0.4632, -0.8644, -0.8555, -0.2135, -0.6025,  0.9812,  0.1557,\n          -0.7758, -0.8984,  0.0668, -0.1028, -0.2639, -0.9575, -0.2613, -0.0081,\n           0.7061,  0.1248,  0.3213, -0.7015,  0.2224, -0.1571,  0.4081,  0.7209,\n          -0.9269, -0.4759,  0.3689, -0.4295, -0.4766, -0.9620,  0.9661, -0.3418,\n           0.3846,  1.0000, -0.1515, -0.8529,  0.6180,  0.2093, -0.1310,  1.0000,\n           0.6444, -0.9701, -0.6138,  0.5510, -0.4863, -0.5596,  0.9992, -0.2357,\n          -0.2975,  0.1004,  0.9670, -0.9839,  0.9649, -0.8907, -0.9589,  0.9602,\n           0.9246, -0.3731, -0.6682,  0.1201, -0.7245,  0.2155, -0.9658,  0.5681,\n           0.5186, -0.1190,  0.8704, -0.8189, -0.5976,  0.2754, -0.4143,  0.2596,\n           0.7457,  0.4388, -0.2805,  0.1202, -0.3369, -0.1061, -0.9752,  0.1715,\n           1.0000,  0.0297, -0.0281, -0.4214, -0.0357, -0.1393,  0.4606,  0.5485,\n          -0.2347, -0.8504,  0.3279, -0.9675, -0.9827,  0.7556,  0.1367, -0.3560,\n           1.0000,  0.3737,  0.1516,  0.2162,  0.8064,  0.0889,  0.6531,  0.6651,\n           0.9729, -0.2619,  0.6603,  0.8313, -0.6553, -0.3012, -0.6386, -0.0148,\n          -0.8941,  0.1146, -0.9509,  0.9573,  0.5843,  0.3169,  0.2516,  0.2392,\n           1.0000, -0.1167,  0.6576, -0.4562,  0.8543, -0.9956, -0.8526, -0.3992,\n          -0.0519, -0.4381, -0.2552,  0.2173, -0.9650,  0.4967,  0.2296, -0.9872,\n          -0.9854,  0.2189,  0.8200,  0.0550, -0.9027, -0.6717, -0.6041,  0.1930,\n          -0.2106, -0.9282,  0.1541, -0.1871,  0.4484, -0.2329,  0.5890,  0.7098,\n           0.5551, -0.2941, -0.1778, -0.1581, -0.8643,  0.8256, -0.8224, -0.6277,\n          -0.1912,  1.0000, -0.3874,  0.7420,  0.7832,  0.6947, -0.1726,  0.2275,\n           0.7798,  0.2235, -0.6840, -0.4765, -0.7018, -0.3786,  0.6919, -0.0551,\n           0.6408,  0.7417,  0.4776,  0.1593, -0.0956, -0.0076,  0.9989, -0.0914,\n          -0.0400, -0.5065, -0.0235, -0.3434, -0.5037,  1.0000,  0.2676, -0.0879,\n          -0.9848, -0.4846, -0.9030,  1.0000,  0.8318, -0.7886,  0.6457,  0.2835,\n          -0.1847,  0.8000, -0.1766, -0.2928,  0.2154,  0.1342,  0.9490, -0.5406,\n          -0.9627, -0.6708,  0.3906, -0.9596,  0.9986, -0.5213, -0.1886, -0.4186,\n           0.1629,  0.6327, -0.1375, -0.9806, -0.1922,  0.2238,  0.9493,  0.2003,\n          -0.6407, -0.9137,  0.3593,  0.4412, -0.6014, -0.9209,  0.9515, -0.9844,\n           0.4333,  1.0000,  0.4262, -0.6058,  0.0762, -0.5610,  0.2879,  0.2575,\n           0.6795, -0.9534, -0.3422, -0.1440,  0.2419, -0.0827,  0.0861,  0.5914,\n           0.1628, -0.5543, -0.5897, -0.0712,  0.4609,  0.7794, -0.3303, -0.1901,\n           0.0985, -0.1683, -0.9158, -0.2656, -0.3308, -0.9998,  0.8316, -1.0000,\n           0.0730, -0.0445, -0.2291,  0.8376,  0.2858,  0.3189, -0.7196, -0.5562,\n           0.6179,  0.7194, -0.1950,  0.1856, -0.7089,  0.1706, -0.0363,  0.0927,\n          -0.3186,  0.8381, -0.1748,  1.0000,  0.1575, -0.6732, -0.9749,  0.2310,\n          -0.2699,  1.0000, -0.9263, -0.9417,  0.3067, -0.6717, -0.8441,  0.3057,\n           0.1691, -0.6650, -0.6412,  0.9577,  0.9319, -0.6354,  0.3161, -0.3202,\n          -0.3484,  0.0650,  0.3785,  0.9805,  0.2146,  0.8812,  0.1865,  0.1208,\n           0.9666,  0.2868,  0.5281, -0.0096,  1.0000,  0.2653, -0.9007,  0.4179,\n          -0.9816, -0.2378, -0.9568,  0.2591,  0.1992,  0.8892, -0.1504,  0.9560,\n          -0.1502, -0.0012, -0.1874,  0.0083,  0.3941, -0.9189, -0.9823, -0.9822,\n           0.3983, -0.4018, -0.0601,  0.1797,  0.1427,  0.3701,  0.4015, -1.0000,\n           0.9311,  0.4155,  0.8236,  0.9400,  0.5101,  0.3197,  0.2572, -0.9815,\n          -0.9717, -0.3451, -0.2015,  0.7119,  0.5982,  0.8706,  0.3487, -0.5003,\n          -0.2890, -0.0533, -0.5967, -0.9901,  0.3677, -0.1926, -0.9682,  0.9573,\n          -0.0542, -0.1982,  0.1948, -0.4994,  0.9433,  0.7541,  0.4190,  0.1231,\n           0.5262,  0.8646,  0.9567,  0.9771, -0.6141,  0.8475, -0.2275,  0.4347,\n           0.4988, -0.9460,  0.1976,  0.3339, -0.4076,  0.2107, -0.1882, -0.9747,\n           0.3138, -0.3396,  0.5244, -0.3769,  0.1480, -0.3856, -0.1060, -0.6954,\n          -0.4961,  0.6654,  0.4441,  0.8817,  0.6919, -0.1072, -0.5519, -0.1474,\n          -0.6456, -0.9095,  0.9075,  0.0175, -0.3469,  0.4021, -0.1442,  0.6139,\n          -0.0221, -0.3936, -0.3833, -0.7945,  0.8564, -0.2575, -0.5593, -0.5533,\n           0.4711,  0.3516,  0.9996, -0.6315, -0.5978, -0.1363, -0.2678,  0.4151,\n          -0.2647, -1.0000,  0.2920, -0.0362,  0.4577, -0.1940,  0.5052, -0.2000,\n          -0.9781, -0.1907,  0.1943,  0.3881, -0.4693, -0.2216,  0.5888,  0.6177,\n           0.7697,  0.8473, -0.2685,  0.2412,  0.6515, -0.4287, -0.6730,  0.9264]],\n        grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None))"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"## 모델 불러오기\n- 모델 : 바디와 헤드로 구분","metadata":{}},{"cell_type":"code","source":"# https://huggingface.co/klue/roberta-base\nroberta_model = AutoModel.from_pretrained('klue/roberta-base')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:54:11.237949Z","iopub.execute_input":"2025-02-04T07:54:11.238145Z","iopub.status.idle":"2025-02-04T07:54:11.383882Z","shell.execute_reply.started":"2025-02-04T07:54:11.238128Z","shell.execute_reply":"2025-02-04T07:54:11.383337Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\n# 바디와 헤드가 포함된 모델\n# https://huggingface.co/SamLowe/roberta-base-go_emotions\nroberta_go_emotions_model = AutoModelForSequenceClassification.from_pretrained('SamLowe/roberta-base-go_emotions')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:54:11.384699Z","iopub.execute_input":"2025-02-04T07:54:11.384893Z","iopub.status.idle":"2025-02-04T07:54:11.480016Z","shell.execute_reply.started":"2025-02-04T07:54:11.384876Z","shell.execute_reply":"2025-02-04T07:54:11.479476Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# 헤드가 없는 모델을 목적에 의해 불러오기\n\nroberta_classification_model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-base')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:54:11.480781Z","iopub.execute_input":"2025-02-04T07:54:11.481054Z","iopub.status.idle":"2025-02-04T07:54:11.583283Z","shell.execute_reply.started":"2025-02-04T07:54:11.481027Z","shell.execute_reply":"2025-02-04T07:54:11.582587Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"roberta_go_emotions_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:54:11.583989Z","iopub.execute_input":"2025-02-04T07:54:11.584249Z","iopub.status.idle":"2025-02-04T07:54:11.589769Z","shell.execute_reply.started":"2025-02-04T07:54:11.584220Z","shell.execute_reply":"2025-02-04T07:54:11.589122Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=28, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"roberta_classification_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:54:11.590520Z","iopub.execute_input":"2025-02-04T07:54:11.590776Z","iopub.status.idle":"2025-02-04T07:54:11.607127Z","shell.execute_reply.started":"2025-02-04T07:54:11.590728Z","shell.execute_reply":"2025-02-04T07:54:11.606536Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}